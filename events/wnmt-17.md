---
autogenerated: true
autogenerated_from: events.json
parent: Events
nav_order: 2017
layout: event
title: WNMT 17
end_date: '2017-08-04'
future_tense_opening_paragraph: The Workshop on Neural Machine Translation and Generation
  (<strong>WNMT 17</strong>) will take place in Vancouver, Canada on 04 August, 2017.
past_tense_opening_paragraph: The Workshop on Neural Machine Translation and Generation
  (<strong>WNMT 17</strong>) took place in Vancouver, Canada on 04 August, 2017.
name: WNMT 17
id: wnmt-17
description: Workshop on Neural Machine Translation and Generation
start_date: '2017-08-04'
location: Vancouver, Canada
organizer:
  type: Person
  url: https://sites.google.com/site/acl17nmt/organization
links:
- https://sites.google.com/site/acl17nmt/overview
one_day_schedule:
- start_time: '9:30'
  title: Welcome and opening remarks
- start_time: '9:40'
  title: 'Keynote 1 <br>The Neural Noisy Channel: Generative Models for Sequence to
    Sequence Modeling <br>Chris Dyer'
- start_time: '10:30'
  title: ‚òïÔ∏è
- start_time: '11:00'
  title: Keynote 2 <br>Challenges in Neural Document Generation <br>Alexander Rush
- start_time: '11:50'
  title: 'Best Paper Session     '
- title: Outstanding paper <br><a href='https://aclanthology.org/W17-3201.pdf'>An
    Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation</a>
    <br>Raphael Shu, Hideki Nakayama
- title: Best Paper <br><a href='https://aclanthology.org/W17-3203.pdf'>Stronger Baselines
    for Trustable Results in Neural Machine Translation</a> <br>Michael Denkowski,
    Graham Neubig
- start_time: '12:20'
  title: üç¥
- start_time: '13:40'
  title: Keynote 3 <br>What is Neural MT Learning? <br>Kevin Knight
- start_time: '14:30'
  title: Keynote 4 <br>Google's Neural Machine Translation system <br>Quoc Le
- start_time: '15:20'
  title: Poster session
- start_time: '15:30'
  title: Poster session (continued) <br>‚òïÔ∏è
- title: <a href='https://aclanthology.org/W17-3201.pdf'>An Empirical Study of Adequate
    Vision Span for Attention-Based Neural Machine Translation</a> <br>Raphael Shu,
    Hideki Nakayama
- title: <a href='https://aclanthology.org/W17-3202.pdf'>Analyzing Neural MT Search
    and Model Performance</a> <br>Jan Niehues, Eunah Cho, Thanh-Le Ha, Alex Waibel
- title: <a href='https://aclanthology.org/W17-3203.pdf'>Stronger Baselines for Trustable
    Results in Neural Machine Translation</a> <br>Michael Denkowski, Graham Neubig
- title: <a href='https://aclanthology.org/W17-3204.pdf'>Six Challenges for Neural
    Machine Translation</a> <br>Philipp Koehn, Rebecca Knowles
- title: <a href='https://aclanthology.org/W17-3205.pdf'>Cost Weighting for Neural
    Machine Translation Domain Adaptation</a> <br>Boxing Chen, Colin Cherry, George
    Foster, Samuel Larkin
- title: <a href='https://aclanthology.org/W17-3206.pdf'>Detecting Untranslated Content
    for Neural Machine Translation</a> <br>Isao Goto, Hideki Tanaka
- title: ' <a href=''https://aclanthology.org/W17-3207.pdf''>Beam Search Strategies
    for Neural Machine Translation</a> <br>Markus Freitag, Yaser Al-Onaizan'
- title: ' <a href=''https://aclanthology.org/W17-3208.pdf''>An Empirical Study of
    Mini-Batch Creation Strategies for Neural Machine Translation</a> <br>Makoto Morishita,
    Yusuke Oda, Graham Neubig, Koichiro Yoshino, Katsuhito Sudoh, Satoshi Nakamura'
- title: <a href='https://aclanthology.org/W17-3209.pdf'>Detecting Cross-Lingual Semantic
    Divergence for Neural Machine Translation</a> <br>Marine Carpuat, Yogarshi Vyas,
    Xing Niu
- title: Domain Aware Neural Dialogue System (extended abstract) <br>Sajal Choudhary,
    Prerna Srivastava, Joao Sedoc, Lyle Ungar
- title: Interactive Beam Search for Visualizing Neural Machine Translation (extended
    abstract) <br>Jaesong Lee, JoongHwi Shin, Jun-Seok Kim
- title: Graph Convolutional Encoders for Syntax-aware Neural Machine Translation
    (extended abstract) <br>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani,
    Khalil Sima‚Äôan
- title: Towards String-to-Tree Neural Machine Translation (cross-submission) <br>Roee
    Aharoni, Yoav Goldberg
- title: What do Neural Machine Translation Models Learn about Morphology? (cross-submission)
    <br>Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James Glass
- title: Trainable Greedy Decoding for Neural Machine Translation (cross-submission)
    <br>Jiatao Gu, Kyunghyun Cho, Victor O.K. Li
- start_time: '16:10'
  title: Panel Discussion <br>Chris Dyer, Alexander Rush, Kevin Knight, Quoc Le, Kyunghuyn
    Cho
- start_time: '17:30'
  title: Closing remarks
seo:
  type: Event
  name: WNMT 17
  description: Workshop on Neural Machine Translation and Generation
  startDate: '2017-08-04'
  endDate: '2017-08-04'
  eventAttendanceMode: OfflineEventAttendanceMode
  eventStatus: EventScheduled
  location:
    type: Place
    name: Vancouver, Canada
  organizer:
    type: Person
    name: null
    url: https://sites.google.com/site/acl17nmt/organization

---
### Tasks

- Efficiency
- Accuracy

### Keynotes

#### ‚ÄúThe Neural Noisy Channel: Generative Models for Sequence to Sequence Modeling‚Äù, Chris Dyer

> The first statistical models of translation relied on Bayes' rule to factorize the probability of an output translation given an input into two component probabilities: a target language model prior probability (how likely is a candidate output?), and an inverse translation probability (how likely is the observed input given a candidate output?). Although this factorization has largely been abandoned in favor of discriminative models that directly estimate the probability of producing an output translation given an input, these discriminative models suffer from a number of problems, including undesirable explaining-away effects during training (e.g., label bias), and difficulty learning from unpaired samples in training. In contrast, generative models based on the Bayes' rule factorization must produce outputs that explain their inputs, and training with unpaired samples (i.e., target language monolingual corpora) is straightforward. I discuss the challenges and opportunities afforded by generative models of sequence to sequence transduction, reporting results on machine translation and abstractive summarization.

#### ‚ÄúChallenges in Neural Document Generation‚Äù, Alexander Rush

> Advances in neural machine translation have led to optimism for natural language generation in tasks such as summarization and dialogue, but it has been difficult to quantify what challenges remain in neural NLG. In this talk, I will discuss recent work on long-form data-to-document generation using a new dataset pairing comprehensive basketball game statistics with full game descriptions, a classic NLG task. While state-of-the-art NMT systems produce fluent output on this task, the generated documents are clearly insufficient and suffer from basic issues in discourse, reference, and referring expression generation. Recent tricks such as copy and coverage lead to clear improvements, but results for end-to-end generation are not yet competitive for long-form documents. Overall, neural document generation presents a difficult but interesting challenge that may require different techniques than standard NMT.

#### ‚ÄúWhat is Neural MT Learning?‚Äù, Kevin Knight

> In this talk, I will observe what neural MT decides to extract from source sentences, as a by-product of its end-to-end training. I will also speculate about the power of neural MT-style networks, both in general and with respect to how they are currently trained.

#### ‚ÄúGoogle's Neural Machine Translation system‚Äù, Quoc Le

> I will talk about the history of neural machine translation at Google and some of our recent work on deploying neural machine translation at scale.
