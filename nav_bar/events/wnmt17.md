---
parent: Events
title: WNMT17
description: Workshop on Neural Machine Translation and Generation
location: Vancouver, Canada
name: WNMT17
startDate: 2017-08-04

seo:
  type: Event
  name: WNMT17
  description: Workshop on Neural Machine Translation and Generation
  startDate: 2017-08-04
  endDate: 2017-08-04
  eventAttendanceMode: OfflineEventAttendanceMode
  eventStatus: EventScheduled

  location:
    type: Place
    name: Vancouver, Canada

  organizer:
    type: Person
    url: https://sites.google.com/site/acl17nmt/organization
---

The **First Workshop on Neural Machine Translation and Generation** (**WNMT17**) took place on 4 August, 2017, in Vancouver, Canada.

[sites.google.com/site/acl17nmt/overview](https://sites.google.com/site/acl17nmt/overview)

### Tasks

- Efficiency
- Accuracy

### Keynotes

#### ‚ÄúThe Neural Noisy Channel: Generative Models for Sequence to Sequence Modeling‚Äù, Chris Dyer

> The first statistical models of translation relied on Bayes' rule to factorize the probability of an output translation given an input into two component probabilities: a target language model prior probability (how likely is a candidate output?), and an inverse translation probability (how likely is the observed input given a candidate output?). Although this factorization has largely been abandoned in favor of discriminative models that directly estimate the probability of producing an output translation given an input, these discriminative models suffer from a number of problems, including undesirable explaining-away effects during training (e.g., label bias), and difficulty learning from unpaired samples in training. In contrast, generative models based on the Bayes' rule factorization must produce outputs that explain their inputs, and training with unpaired samples (i.e., target language monolingual corpora) is straightforward. I discuss the challenges and opportunities afforded by generative models of sequence to sequence transduction, reporting results on machine translation and abstractive summarization.

#### ‚ÄúChallenges in Neural Document Generation‚Äù, Alexander Rush

> Advances in neural machine translation have led to optimism for natural language generation in tasks such as summarization and dialogue, but it has been difficult to quantify what challenges remain in neural NLG. In this talk, I will discuss recent work on long-form data-to-document generation using a new dataset pairing comprehensive basketball game statistics with full game descriptions, a classic NLG task. While state-of-the-art NMT systems produce fluent output on this task, the generated documents are clearly insufficient and suffer from basic issues in discourse, reference, and referring expression generation. Recent tricks such as copy and coverage lead to clear improvements, but results for end-to-end generation are not yet competitive for long-form documents. Overall, neural document generation presents a difficult but interesting challenge that may require different techniques than standard NMT.

#### ‚ÄúWhat is Neural MT Learning?‚Äù, Kevin Knight

> In this talk, I will observe what neural MT decides to extract from source sentences, as a by-product of its end-to-end training. I will also speculate about the power of neural MT-style networks, both in general and with respect to how they are currently trained.

#### ‚ÄúGoogle's Neural Machine Translation system‚Äù, Quoc Le

> I will talk about the history of neural machine translation at Google and some of our recent work on deploying neural machine translation at scale.


### Schedule


#### Session 1

|     |     |
| --- | --- |
| 09:30 - 09:40 | **Welcome and opening remarks** |
| 09:40 - 10:30 | **Keynote 1** <br>**The Neural Noisy Channel: Generative Models for Sequence to Sequence Modeling** <br>Chris Dyer |
| 10:30 - 11:00 | ‚òïÔ∏è |

#### Session 2

|     |     |
| --- | --- |
| 11:00 - 11:50 | **Keynote 2** <br>Challenges in Neural Document Generation <br>Alexander Rush |
| 11:50 - 12:20 | **Best Paper Session** |
|     | **Outstanding paper** <br>[***An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation***](https://aclanthology.org/W17-3201.pdf) <br>Raphael Shu, Hideki Nakayama |
|     | **Best Paper** <br>[***Stronger Baselines for Trustable Results in Neural Machine Translation***](https://aclanthology.org/W17-3203.pdf) <br>Michael Denkowski, Graham Neubig |
| 12:20 - 13:40 | üç¥ |

#### Session 3

|     |     |
| --- | --- |
| 13:40 - 14:30 | **Keynote 3** <br>**What is Neural MT Learning?** <br>Kevin Knight |
| 14:30 - 15:20 | **Keynote 4** <br>**Google's Neural Machine Translation system** <br>Quoc Le |
| 15:20 - 15:30 | **Poster session** |
| 15:30 - 16:10 | **Poster session** (continued) <br>‚òïÔ∏è |
|     | [***An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation***](https://aclanthology.org/W17-3201.pdf) <br>Raphael Shu, Hideki Nakayama |
|     | [***Analyzing Neural MT Search and Model Performance***](https://aclanthology.org/W17-3202.pdf) <br>Jan Niehues, Eunah Cho, Thanh-Le Ha, Alex Waibel |
|     | [***Stronger Baselines for Trustable Results in Neural Machine Translation***](https://aclanthology.org/W17-3203.pdf) <br>Michael Denkowski, Graham Neubig |
|     | [***Six Challenges for Neural Machine Translation***](https://aclanthology.org/W17-3204.pdf) <br>Philipp Koehn, Rebecca Knowles |
|     | [***Cost Weighting for Neural Machine Translation Domain Adaptation***](https://aclanthology.org/W17-3205.pdf) <br>Boxing Chen, Colin Cherry, George Foster, Samuel Larkin |
|     | [***Detecting Untranslated Content for Neural Machine Translation***](https://aclanthology.org/W17-3206.pdf) <br>Isao Goto, Hideki Tanaka |
|     | [***Beam Search Strategies for Neural Machine Translation***](https://aclanthology.org/W17-3207.pdf) <br>Markus Freitag, Yaser Al-Onaizan |
|     | [***An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation***](https://aclanthology.org/W17-3208.pdf) <br>Makoto Morishita, Yusuke Oda, Graham Neubig, Koichiro Yoshino, Katsuhito Sudoh, Satoshi Nakamura |
|     | [***Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation***](https://aclanthology.org/W17-3209.pdf) <br>Marine Carpuat, Yogarshi Vyas, Xing Niu |
|     | **Domain Aware Neural Dialogue System (extended abstract)** <br>Sajal Choudhary, Prerna Srivastava, Joao Sedoc, Lyle Ungar |
|     | **Interactive Beam Search for Visualizing Neural Machine Translation (extended abstract)** <br>Jaesong Lee, JoongHwi Shin, Jun-Seok Kim |
|     | **Graph Convolutional Encoders for Syntax-aware Neural Machine Translation (extended abstract)** <br>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima‚Äôan |
|     | **Towards String-to-Tree Neural Machine Translation (cross-submission)** <br>Roee Aharoni, Yoav Goldberg |
|     | **What do Neural Machine Translation Models Learn about Morphology? (cross-submission)** <br>Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James Glass |
|     | **Trainable Greedy Decoding for Neural Machine Translation (cross-submission)** <br>Jiatao Gu, Kyunghyun Cho, Victor O.K. Li |

#### Session 4

|     |     |
| --- | --- |
| 16:10 - 17:30 | **Panel Discussion** <br>Chris Dyer, Alexander Rush, Kevin Knight, Quoc Le, Kyunghuyn Cho |
| 17:30 - 17:40 | **Closing remarks** |
