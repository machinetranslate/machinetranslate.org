import os
import re

import enchant
# import nltk
# from nltk.tokenize import sent_tokenize
# import spacy


# from find_missing_links import walk_directory


# def check_abbreviations(file_path):
#     with open(file_path, 'r', encoding='utf-8') as f:
#         f.seek(0)
#         next(f)
#         is_frontmatter = True
#         in_html_block = False

#         content = f.read()
#         for line in content.splitlines():

#             # Return if autogenerated
#             if line.strip() == "autogenerated: true":
#                 return

#             # Skip if frontmatter
#             if is_frontmatter:
#                 if line.startswith('---'):
#                     is_frontmatter = False
#                 continue
            
#             # Skip HTML
#             if '<' in line and '>' in line:
#                 in_html_block = True

#             if in_html_block:
#                 continue

#             _line = line.lower()

#             # Remove existing Markdown links and Liquid tags
#             _line = re.sub(r'\[(.*?)\]\([^)]*?\)|\{\{[^}]*\}\}|{%[^\n]*%}', ' ... ', _line)
#             # print(_line)
#             words = re.findall(r'\b\w+\b', line)
#             for word in words:
#                 # print(word)
#                 if word.isupper() and len(word) > 1 and word != file_path.split('.md')[-1]:
#                     # print(word)
#                     print(f" abbv: {word} File: {file_path}, Content: {line.strip()}")


def walk_directory(directory, exclude_dirs):
    for root, dirs, files in os.walk(directory):
        # Check if any excluded directory is present in the current path
        if any(exclude_dir in root for exclude_dir in exclude_dirs):
            continue  # Skip the current directory and its subdirectories

        # Remove directories that start with '.' or '_'
        dirs[:] = [d for d in dirs if not d.startswith(('.', '_'))]

        for file in files:
            if file.endswith('.md'):
                yield os.path.join(root, file)

def check_abbreviations(file_path):
    ff = []
    for root, dirs, files in os.walk('../../'):
        
        # Check if any excluded directory is present in the current path
        if any(exclude_dir in root for exclude_dir in ['events', 'vendor']):
            continue  # Skip the current directory and its subdirectories

        # Remove directories that start with '.' or '_'
        dirs[:] = [d for d in dirs if not d.startswith(('.', '_'))]
        ff.extend(files)

    with open(file_path, 'r', encoding='utf-8') as f:
        f.seek(0)
        next(f)
        is_frontmatter = True
        in_html_block = False

        content = f.read()
        for line_number, line in enumerate(content.splitlines(), start=1):

            # Return if autogenerated
            if line.strip() == "autogenerated: true":
                continue

            # Skip if frontmatter
            if is_frontmatter:
                if line.startswith('---'):
                    is_frontmatter = False
                continue
            
            # Skip HTML
            if '<' in line and '>' in line:
                in_html_block = True

            if in_html_block:
                continue

            _line = line.lower()

            # Remove existing Markdown links and Liquid tags
            _line = re.sub(r'\[(.*?)\]\([^)]*?\)|\{\{[^}]*\}\}|{%[^\n]*%}', ' ... ', _line)

            # Extract words and check for capital words
            words = re.findall(r'\b\w+\b', line)
            # words = re.findall(r'\b[\w.]+\b', line)
            for word in words:
                if word.isupper() and len(word) > 1:
                    if f'{word.lower()}.md' in ff:
                        continue
                    print(f"ABBV: {word}  File: {file_path}, Content: {line.strip()}\n")
                    input()

# --------------------------------------------------------------
# ----------------------------------------------------------------


                                # print(ff)

            #         word_path = file_path.replace(file_path.split('/')[-1], f'{word.lower()}.md')
            #         if not os.path.exists(word_path):
                    
                    # if os.path.exists(word_path):
                    #     continue
                    # else:
                        # print(f"ABBV: {word}  File: {file_path}, Content: {line.strip()}\n")
                        # input()


    # Check for abbreviations (e.g., "U.S.")
    # abbreviation_matches = re.findall(r'\b[A-Za-z]\.[A-Za-z]\.', content)
    # abbreviation_matches = [match for match in abbreviation_matches if match.lower() not in ['e.g.', 'i.e.', 'a.i.']]

    # if abbreviation_matches:
    #     return f"Abbreviations found in {file_path}: {', '.join(abbreviation_matches)}"

    # return None  # No abbreviations found


# def check_abbreviations(file_path):
#     with open(file_path, 'r', encoding='utf-8') as file:
#         content = file.read()

#     # Check for abbreviations (e.g., "U.S.", "TMS", etc.)
#     # abbreviation_matches = re.findall(r'\b(?:[A-Za-z]\.|[A-Za-z]{2,})\b', content)
#     abbreviation_matches = re.findall(r'\b(?:[A-Za-z]\.|[A-Za-z]{2,})\b', content)
#     abbreviation_matches = [match for match in abbreviation_matches if match.lower() not in ['e.g.', 'i.e.', 'a.i.']]

#     if abbreviation_matches:
#         return f"Abbreviations found in {file_path}: {', '.join(abbreviation_matches)}"

#     return None 

def check_spellings(file_path, article_name):
    unlinked_word_pattern = re.compile(r'\b(?:' + '|'.join(map(re.escape, article_name)) + r')\b', re.IGNORECASE)
    dictionary = enchant.Dict("en_US")
    uk = enchant.Dict('en_GB')
    with open(file_path, 'r', encoding='utf-8') as f:
        if "autogenerated: true" not in f.read():
            f.seek(0)
            next(f)
            is_frontmatter = True

            for line in f:
                if is_frontmatter:
                    if line.startswith('---'):
                        is_frontmatter = False
                    continue

                # Identify and exclude links
                # link_matches = re.finditer(r'\[(.*?)\]\([^)]*?\)', line)
                # link_ranges = [(match.start(), match.end()) for match in link_matches]
                # line_without_links = ''.join([' ... ' if (start, end) in link_ranges else line[start:end] for start, end in zip([0] + [end for _, end in link_ranges], [start for start, _ in link_ranges] + [None])])
                _line = line

                line_without_links = re.sub(r'\[(.*?)\]\([^)]*?\)|\{\{[^}]*\}\}|{%[^\n]*%}|<[^>]*>', ' ... ', _line)

                for word in line_without_links.split():
                    # if dictionary.check(i) != uk.check(i):
                     # Extract only letters from the word
                    raw_word = re.sub(r'^[^a-zA-Z\'-]*|[^a-zA-Z\'-]*$', '', word)
                    # words = re.findall(r'\b(?:[a-zA-Z]+(?:\'[a-zA-Z]+)?(?:-[a-zA-Z]+)?|[\'-])\b', word)
                    # raw_word = ''.join(words)



                    if raw_word:
                        if not uk.check(raw_word) and dictionary.check(raw_word):
                    # if not uk.check(i):
                        
                    # if not uk.check(i):
                    # print(i)
                    # input()
                # Apply the regex to the line without links
                # for match in line_without_links:
                #     print(match)
                #     input()
                    # article_name = match.group()
                    # suggestion_key = (article_name, file_path)
                    # if (
                    #     f'{article_name}.md' != file_path.split('/')[-1] and
                    #     suggestion_key not in suggested_set
                    # ):
                    #     unlinked_terms.append({'term': article_name, 'file_path': file_path, 'line': line})
                    #     suggested_set.add(suggestion_key)

    # with open(file_path, 'r', encoding='utf-8') as file:
    #     content = file.readline().split()
    #     for i in content:      
    #         print(i)
    #         input() 
            

    # Check for US-specific spellings
    # us_spelling_matches = re.findall(r'\b([A-Za-z]+)our\b', content)
    # for word in us_spelling_matches:
    #     if dictionary.check(word):
    #         return f"US-specific spelling found in {file_path}: {word}"
    # us_spelling_matches = re.findall(r'\b([A-Za-z]+)\b', content)
    # for word in us_spelling_matches:
    #     if dictionary.check(word) != word:
                                print(f"US-specific spelling found in {file_path}: {raw_word} -> suggestions: {uk.suggest(raw_word)} \n")
                                print(30 * '-')
                                input()

    return None  # No US-specific spellings found

# def check_newlines(file_path):
#     with open(file_path, 'r', encoding='utf-8') as file:
#         # content = file.read()
#         if "autogenerated: true" not in file.read():
#             file.seek(0)
#             next(file)
#             is_frontmatter = True

#             for line in file:
#                 if is_frontmatter:
#                     if line.startswith('---'):
#                         is_frontmatter = False
#                     continue
#                 link_matches = re.finditer(r'\[(.*?)\]\([^)]*?\)', line)
#                 link_ranges = [(match.start(), match.end()) for match in link_matches]
#                 line_without_links = ''.join([' ... ' if (start, end) in link_ranges else line[start:end] for start, end in zip([0] + [end for _, end in link_ranges], [start for start, _ in link_ranges] + [None])])
#                 line_without_links = re.sub(r'\[(.*?)\]\([^)]*?\)|\{\{[^}]*\}\}|{%[^\n]*%}|<[^>]*>', ' ... ', line_without_links)
#                 # Check for newline after each sentence, and two newlines after space
#                 sentences = re.split(r'(?<=[.!?])\s*\n{2,}', line_without_links)
#                 for sentence in sentences:
#                     if not re.match(r'^[A-Z]', sentence):
#                         print(f"Invalid sentence structure in {file_path}: {sentence}")
#                         input()

#     return None  # No issues found


# def check_newlines(file_path):
#     with open(file_path, 'r', encoding='utf-8') as file:
#         # content = file.read()
#         if "autogenerated: true" not in file.read():
#             file.seek(0)
#             next(file)
#             is_frontmatter = True

#             for content in file:
#                 if is_frontmatter:
#                     if content.startswith('---'):
#                         is_frontmatter = False
#                     continue

#     # Check for newline after each sentence, two after space
#                 sentences = re.split(r'(?<=[.!?]) +', content)
#                 for sentence in sentences:
#                     # print(sentence)
#                     if not re.match(r'^[A-Z]', sentence):
#                         return f"Invalid sentence structure in : {sentence}"

#     return None  # No issues found
# EXCLUDE_DIRS = ['events', 'vendor']

# def main():
#     for root, dirs, files in os.walk('../..'):
#         if all(exclude_dir not in root for exclude_dir in EXCLUDE_DIRS):
#             dirs[:] = [d for d in dirs if not d.startswith(('.', '_'))]
#         for file in files:
#             # print(file)
#             if file.endswith('.md'):
#                 file_path = os.path.join(root, file)
#                 article_name = file.split('.md')[0]
#                 for checker in [check_abbreviations]:
#                 # for checker in [check_spellings]:
#                     # result = checker(file_path, article_name)
#                     result = checker(file_path)
#                     if result:
#                         input()
#                         print(result)

# if __name__ == "__main__":
#     main()

#-------------------------------#



def check_newlines(file_path):
    # Read the content of the Markdown file
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()

    # Use regular expression to find sentences without newline after them
    matches = re.finditer(r'(?<=[.!?])\s*', text, flags=re.MULTILINE)
    
    for match in matches:
        if match.group() != '\n':
            start = max(0, match.start() - 20)  # print 20 characters before the sentence
            end = min(len(text), match.end() + 20)  # print 20 characters after the sentence
            sentence = text[start:end]
            print(f"Warning: No newline after sentence '{sentence}' at position {match.end()} in {file_path}")












#-------------------------------NLTK#


EXCLUDE_DIRS = ['events', 'vendor']
DIR = '../../'

def main():
    # for root, dirs, files in os.walk('../..'):
    #     if any(exclude_dir in root for exclude_dir in EXCLUDE_DIRS):
    #         continue  # Skip the current directory and its subdirectories

    #     # Remove directories that start with '.' or '_'
    #     dirs[:] = [d for d in dirs if not d.startswith(('.', '_'))]
        article_paths = list(walk_directory(directory=DIR, exclude_dirs=EXCLUDE_DIRS))
        for file in article_paths:
            # print(file)
        # for file in files:
        #     if file.endswith('.md'):
            # file_path = os.path.join(root, file)
            article_name = file.split('.md')[0]
            for checker in [check_abbreviations]:
                result = checker(file)
                if result:
                    # input()
                    print(result)

if __name__ == "__main__":
    main()
    # check_abbreviations()
    # check_newlines('../../building-and-research/building-and-research.md')